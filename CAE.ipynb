{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from enum import Enum\n",
    "import torch\n",
    "import random\n",
    "import tqdm\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Robot class\n",
    "\n",
    "Given a maze matrix, the robot will start from the start state and go to the target state.\n",
    "\n",
    ".train() method will train the robot.\n",
    ".q_matrix to get the q matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Robot:\n",
    "    \"\"\"\n",
    "    Given a maze matrix, the robot will start from the start state and go to the target state.\n",
    "    \"\"\"\n",
    "    def __init__(self, start_state: tuple[int,int], target_state: tuple[int,int], maze_matrix: torch.Tensor, \n",
    "                 actions: torch.Tensor = torch.tensor([[0, -1], [0, 1], [-1, 0], [1, 0]], dtype=torch.int8),\n",
    "                 reward_matrix: torch.Tensor = None, restrict_matrix: torch.Tensor = None, q_matrix: torch.Tensor = None, \n",
    "                 max_step = 100, greedy_rate = 0.7, discount = 0.9, lr = 0.1, \n",
    "                 wall_reward = -100, target_reward = 100):\n",
    "        \n",
    "        self.start_state = torch.tensor(start_state)\n",
    "        self.target_state = torch.tensor(target_state)\n",
    "        self.maze_matrix = maze_matrix\n",
    "        self.actions = actions\n",
    "        self.max_step = max_step\n",
    "        self.greedy_rate = greedy_rate\n",
    "        self.discount = discount\n",
    "        self.lr = lr\n",
    "        \n",
    "        self.n_action = actions.size(0)\n",
    "        \n",
    "        if reward_matrix is None:\n",
    "            reward_matrix = -torch.ones_like(maze_matrix, dtype=torch.int8)\n",
    "            reward_matrix[maze_matrix] = wall_reward\n",
    "            reward_matrix[target_state[0], target_state[1]] = target_reward\n",
    "        self.reward_matrix = reward_matrix\n",
    "            \n",
    "        if restrict_matrix is None:\n",
    "            restrict_matrix = torch.zeros_like(reward_matrix, dtype=torch.bool)\n",
    "        self.restrict_matrix = restrict_matrix            \n",
    "        \n",
    "        if q_matrix is None:\n",
    "            q_matrix = torch.zeros_like(reward_matrix, dtype=torch.float32).repeat(self.n_action,1,1)\n",
    "        self.q_matrix = q_matrix\n",
    "    \n",
    "    def choose_action_id(self, state: torch.Tensor, greedy_rate: float = None) -> int:\n",
    "        \"\"\"\n",
    "        Choose an action based on the given state and greedy rate.\n",
    "        \"\"\"\n",
    "        \n",
    "        if greedy_rate is None:\n",
    "            greedy_rate = self.greedy_rate\n",
    "            \n",
    "        # ramdom choose action\n",
    "        if random.random() > greedy_rate:\n",
    "            return random.randint(0, self.n_action - 1)\n",
    "        # greedy choose action\n",
    "        else:\n",
    "            q_values = self.q_matrix[:, state[0], state[1]] # get q values of all actions\n",
    "            max_value = torch.max(q_values) # get max q value of all actions\n",
    "            max_indices = torch.where(q_values == max_value)[0].tolist() # get indices of max q value\n",
    "            return random.choice(max_indices) # choose one of the max q value\n",
    "    \n",
    "    def update(self, state:torch.Tensor, action_id: int, next_state: torch.Tensor, discount = None, lr = None):\n",
    "        \"\"\"\n",
    "        Update the q matrix based on the given state, action, next state, discount and learning rate.\n",
    "        \"\"\"\n",
    "        if discount is None:\n",
    "            discount = self.discount\n",
    "        if lr is None:\n",
    "            lr = self.lr\n",
    "        \n",
    "        reward = self.reward_matrix[next_state[0], next_state[1]]\n",
    "        q_old = self.q_matrix[action_id, state[0], state[1]]\n",
    "        \n",
    "        if self.maze_matrix[next_state[0], next_state[1]]:\n",
    "            q_new = reward\n",
    "        else:\n",
    "            q_next_max = torch.max(self.q_matrix[:, next_state[0], next_state[1]])\n",
    "            q_new = reward + discount * q_next_max\n",
    "        \n",
    "        self.q_matrix[action_id, state[0], state[1]] += lr * (q_new - q_old)\n",
    "    \n",
    "    def go(self, start_state: torch.Tensor, valid: bool = False, debug: bool = False) -> list:\n",
    "        \"\"\"\n",
    "        Start from the start state and go to the target state.\n",
    "        \"\"\"\n",
    "        states = [start_state]\n",
    "        state = states[-1]\n",
    "        # while not stop_matrix[state[0], state[1]] and len(states) <= MAX_STEP:\n",
    "        while not torch.allclose(state, self.target_state) and len(states) <= self.max_step:\n",
    "            if valid: # if in valid mode, choose the best action\n",
    "                action_id = self.choose_action_id(state, 1)\n",
    "            else:   # if not in valid mode, choose action based on greedy rate\n",
    "                action_id = self.choose_action_id(state)\n",
    "            # Get next state based on the action\n",
    "            next_state = state + self.actions[action_id]\n",
    "            if not valid: # if not in valid mode, update the q matrix\n",
    "                self.update(state, action_id, next_state)\n",
    "\n",
    "            if not self.maze_matrix[next_state[0], next_state[1]] + self.restrict_matrix[next_state[0], next_state[1]]: # if not hit the wall or restricted\n",
    "                state = next_state\n",
    "            else: # if hit the wall\n",
    "                if debug:\n",
    "                    print(f'{next_state}, hit the wall or restricted')\n",
    "            states.append(next_state) # add the next state to the states list\n",
    "        return states\n",
    "    \n",
    "    def train(self, n_epoch: int = 100, valid: bool = False, debug: bool = False) -> list:\n",
    "        \"\"\"\n",
    "        Train the robot for n_epoch times.\n",
    "        \"\"\"\n",
    "        paths = []\n",
    "        for epoch in tqdm.trange(n_epoch):\n",
    "            path = self.go(self.start_state, valid, debug)\n",
    "            paths.append(path)\n",
    "        return paths\n",
    "\n",
    "    def eval(self, debug: bool = False) -> list:\n",
    "        \"\"\"\n",
    "        Evaluate the robot.\n",
    "        \"\"\"\n",
    "        return self.go(self.start_state, True, debug)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConvAutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BiasLayer(nn.Module):\n",
    "    '''\n",
    "    Bias Layer (add bias to individual network nodes/filter positions)\n",
    "    '''\n",
    "    def __init__(self, shape: tuple):\n",
    "        '''\n",
    "        Initialise parameters of bias layer\n",
    "        ---\n",
    "        INPUT\n",
    "        shape: Requisite shape of bias layer\n",
    "        '''\n",
    "        super(BiasLayer, self).__init__()\n",
    "        init_bias = torch.zeros(shape, device=device)\n",
    "        self.bias = nn.Parameter(init_bias, requires_grad=True)\n",
    "\n",
    "    def forward(self, x: torch.tensor)->torch.tensor:\n",
    "        '''\n",
    "        Forward pass\n",
    "        ---\n",
    "        INPUT\n",
    "        x: Input features\n",
    "        ---\n",
    "        OUTPUT\n",
    "        y: Output of bias layer\n",
    "        '''\n",
    "        y=x+self.bias\n",
    "        return y\n",
    "\n",
    "def conv2d_output_dims(x: 'tuple[int,int,int]', layer: nn.Conv2d)->'tuple[int,int,int]':\n",
    "    \"\"\"\n",
    "    Unnecessarily complicated but complete way to\n",
    "    calculate the output depth, height\n",
    "    and width size for a Conv2D layer\n",
    "    ---\n",
    "    INPUT\n",
    "    Args:\n",
    "    x: Input size (depth, height, width)\n",
    "    layer: The Conv2D layer\n",
    "    ---\n",
    "    OUTPUT:\n",
    "    Tuple of out-depth/out-height and out-width\n",
    "    Output shape as given in [Ref]\n",
    "    Ref:\n",
    "    https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n",
    "    \"\"\"\n",
    "    assert isinstance(layer, nn.Conv2d)\n",
    "    p = layer.padding if isinstance(layer.padding, tuple) else (layer.padding,)\n",
    "    k = layer.kernel_size if isinstance(layer.kernel_size, tuple) else (layer.kernel_size,)\n",
    "    d = layer.dilation if isinstance(layer.dilation, tuple) else (layer.dilation,)\n",
    "    s = layer.stride if isinstance(layer.stride, tuple) else (layer.stride,)\n",
    "    in_depth, in_height, in_width = x\n",
    "    out_depth = layer.out_channels\n",
    "    out_height = 1 + (in_height + 2 * p[0] - (k[0] - 1) * d[0] - 1) // s[0]\n",
    "    out_width = 1 + (in_width + 2 * p[-1] - (k[-1] - 1) * d[-1] - 1) // s[-1]\n",
    "    return (out_depth, out_height, out_width)\n",
    "\n",
    "class ConvAutoEncoder(nn.Module):\n",
    "    '''\n",
    "    A Convolutional AutoEncoder\n",
    "    '''\n",
    "    def __init__(self, x_dim: 'tuple[int,int,int]', K: int, nonlinear_ae: bool, nonlinear_std: bool, n_filters: int=10, filter_size: int=2, y_dim: 'tuple[int,int,int]'=None):\n",
    "        '''\n",
    "        Initialize parameters of ConvAutoEncoder\n",
    "        ---\n",
    "        INPUT\n",
    "        x_dim: Input dimensions (channels, height, widths)\n",
    "        K: message length/hidden dimension\n",
    "        nonlinear_ae: are the activations in the autoencoder nonlinear?\n",
    "        nonlinear_std: are the activations in the student nonlinear?\n",
    "        n_filters: Number of filters (number of output channels)\n",
    "        filter_size: Kernel size\n",
    "        '''\n",
    "        super().__init__()\n",
    "        channels, height, widths = x_dim\n",
    "        if y_dim is None:\n",
    "            y_dim = x_dim\n",
    "        y_channels, y_height, y_widths = y_dim\n",
    "\n",
    "        # Encoder input bias layer\n",
    "        self.enc_bias = BiasLayer(x_dim)\n",
    "        # First encoder conv2d layer\n",
    "        #32 different filters -> grid_dim x grid_dim x n_actions to grid_dim+1 x grid_dim+1 x 32\n",
    "        self.enc_conv_1 = nn.Conv2d(channels, n_filters, filter_size, padding=filter_size-1, device=device)\n",
    "        #32 different filters -> grid_dim+1 x grid_dim+1 x 32 to grid_dim+2 x grid_dim+2 x 32\n",
    "        # Output shape of the first encoder conv2d layer given x_dim input\n",
    "        conv_1_shape = conv2d_output_dims(x_dim, self.enc_conv_1)\n",
    "        # Second encoder conv2d layer\n",
    "        self.enc_conv_2 = nn.Conv2d(n_filters, n_filters, filter_size, padding=filter_size-1, device=device) #and here once again 32 different filters?!\n",
    "        # Output shape of the second encoder conv2d layer given conv_1_shape input\n",
    "        conv_2_shape = conv2d_output_dims(conv_1_shape, self.enc_conv_2)\n",
    "        # The bottleneck is a dense layer, therefore we need a flattenning layer\n",
    "        self.enc_flatten = nn.Flatten()\n",
    "        # Conv output shape is (depth, height, width), so the flatten size is:\n",
    "        flat_after_conv = conv_2_shape[0] * conv_2_shape[1] * conv_2_shape[2]\n",
    "        # Encoder Linear layer\n",
    "        self.enc_lin = nn.Linear(flat_after_conv, K, device=device)\n",
    "\n",
    "        # Decoder Linear layer\n",
    "        self.dec_lin = nn.Linear(K, flat_after_conv, device=device)\n",
    "        # Unflatten data to (depth, height, width) shape\n",
    "        self.dec_unflatten = nn.Unflatten(dim=-1, unflattened_size=conv_2_shape)\n",
    "        # First \"deconvolution\" layer\n",
    "        self.dec_deconv_1 = nn.ConvTranspose2d(n_filters, n_filters, filter_size, padding=filter_size-1, device=device)\n",
    "        # Second \"deconvolution\" layer\n",
    "        self.dec_deconv_2 = nn.ConvTranspose2d(n_filters, y_channels, filter_size, padding=filter_size-1, device=device)\n",
    "        # Decoder output bias layer\n",
    "        self.dec_bias = BiasLayer(y_dim)\n",
    "\n",
    "        #booleans marking the nonlinearities\n",
    "        self.nonlinear_ae=nonlinear_ae\n",
    "        self.nonlinear_std=nonlinear_std\n",
    "\n",
    "    def encode(self, q:torch.tensor)->torch.tensor:\n",
    "        '''\n",
    "        first half of autoencoder: encode q-matrix to create the message\n",
    "        ---\n",
    "        INPUT\n",
    "        q: The Q-matrix\n",
    "        ---\n",
    "        OUTPUT\n",
    "        m: The message, i.e. the encoded Q-matrix\n",
    "        '''\n",
    "        m = self.enc_bias(q)\n",
    "\n",
    "        #nonlinear\n",
    "        if self.nonlinear_ae:\n",
    "            m = F.relu(self.enc_conv_1(m))\n",
    "            m = F.relu(self.enc_conv_2(m))\n",
    "        #linear\n",
    "        else:\n",
    "            m=self.enc_conv_1(m)\n",
    "            m=self.enc_conv_2(m)\n",
    "\n",
    "        m = self.enc_flatten(m)\n",
    "        m = self.enc_lin(m)\n",
    "        return m\n",
    "\n",
    "\n",
    "    def decode_ae(self, m:torch.tensor)->torch.tensor:\n",
    "        '''\n",
    "        second half of autoencoder: reconstruct the original q-matrix from the message\n",
    "        ---\n",
    "        INPUT\n",
    "        m: The message\n",
    "        ---\n",
    "        OUTPUT\n",
    "        q: The decoded Q-matrix\n",
    "        '''\n",
    "\n",
    "        #nonlinear\n",
    "        if self.nonlinear_ae:\n",
    "            q = F.relu(self.dec_lin(m))\n",
    "            q = self.dec_unflatten(q)\n",
    "            q = F.relu(self.dec_deconv_1(q))\n",
    "        #linear\n",
    "        else:\n",
    "            q=self.dec_lin(m)\n",
    "            q = self.dec_unflatten(q)\n",
    "            q=self.dec_deconv_1(q)\n",
    "\n",
    "        q = self.dec_deconv_2(q)\n",
    "        q = self.dec_bias(q)\n",
    "        return q\n",
    "\n",
    "\n",
    "    def forward(self, q: torch.tensor)->'tuple[torch.tensor, torch.tensor]':\n",
    "        '''\n",
    "        do a forward pass of the autoencoder, i.e. encoding and decoding, but without the student\n",
    "        ---\n",
    "        INPUT\n",
    "        q: A number of messages combined in one tensor\n",
    "        OUTPUT\n",
    "        m: The student Q-matrices corresponding to the input messages\n",
    "        q_rec: The reconstructed Q-matrix (by the second half of the autoencoder)\n",
    "        '''\n",
    "        m=self.encode(q)\n",
    "        q_rec=self.decode_ae(m)\n",
    "        return m, q_rec\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "batches = torch.load('batches.pt')\n",
    "n_batches = len(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train and test\n",
    "train_test_ratio = 0.8\n",
    "train_batches = batches[:int(n_batches*train_test_ratio)]\n",
    "test_batches = batches[int(n_batches*train_test_ratio):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dimensions\n",
    "maze_tensor, d_tensor, q_matrices_tensor, q_matrix_combo_tensor, target_states, start_states = batches[0]\n",
    "x_dim = tuple(q_matrices_tensor[0].shape) # (depth, height, width) = (4*2, 9, 9)\n",
    "y_dim = tuple(q_matrix_combo_tensor[0].shape) # (depth, height, width) = (4, 9, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train autoencoder\n",
    "n_epoches = 100\n",
    "\n",
    "autoencoder = ConvAutoEncoder(x_dim, 30, True, True, y_dim=y_dim).to(device)\n",
    "optimizer = torch.optim.Adam(autoencoder.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "for epoch in range(n_epoches):\n",
    "    for batch in train_batches:\n",
    "        maze_tensor, d_tensor, q_matrices_tensor, q_matrix_combo_tensor, target_states, start_states = batch\n",
    "        optimizer.zero_grad()\n",
    "        m, q_rec = autoencoder(q_matrices_tensor)\n",
    "        loss = criterion(q_rec, q_matrix_combo_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'epoch: {epoch}, loss: {loss.item()}')\n",
    "\n",
    "# torch.save(autoencoder, 'autoencoder.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autoencoder = torch.load('autoencoder.pt')\n",
    "\n",
    "from statistics import mean\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_batches:\n",
    "        maze_tensor, d_tensor, q_matrices_tensor, q_matrix_combo_tensor, target_states, start_states = batch\n",
    "        m, q_rec = autoencoder(q_matrices_tensor)\n",
    "        loss = criterion(q_rec, q_matrix_combo_tensor)\n",
    "        min_len_deltas = []\n",
    "        \n",
    "        start_states = [tuple(start_state) for start_state in start_states]\n",
    "        for q_matrix_combo, target_state in zip(q_rec, target_states):\n",
    "            target_state = tuple(target_state)\n",
    "            lens = []\n",
    "            for start_state in start_states:\n",
    "                robot = Robot(start_state, target_state, maze_tensor, q_matrix=q_matrix_combo)\n",
    "                path = robot.eval()\n",
    "                lens.append(len(path)-1)\n",
    "                # print(path)\n",
    "            min_len = min(lens)\n",
    "            min_len_delta = int(min_len - d_tensor[target_state[0], target_state[1]])\n",
    "            min_len_deltas.append(min_len_delta)\n",
    "        print(f'loss: {loss.item()}, min_len_deltas_avg: {mean(min_len_deltas)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlclass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
