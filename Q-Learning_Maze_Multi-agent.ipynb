{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the sample maze\n",
    "\n",
    "Set up a maze in size of 10 × 10, and represent the maze in reward matrix. The reward matrix is a 10 × 10 matrix, and the reward of each grid is as follows:\n",
    "\n",
    "- The reward of the grids in the border is -100.\n",
    "- The reward of the grids in the middle is -1.\n",
    "- The reward of the destination is 100.\n",
    "\n",
    "Set up a stop matrix, which is a 10 × 10 bool matrix. The stop matrix is used to determine whether the agent can stop at the grid. \n",
    "\n",
    "The end point is (4,5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_matrix = torch.tensor(data=[[-100,-100,-100,-100,-100,-100,-100,-100,-100,-100],\n",
    "                                   [-100, -1, -100,  -1,-100,  -1,-100,  -1,-100,-100],\n",
    "                                   [-100, -1,   -1,  -1,  -1,  -1,-100,  -1,  -1,-100],\n",
    "                                   [-100, -1, -100,  -1,-100,  -1,-100,  -1,-100,-100],\n",
    "                                   [-100, -1,   -1,  -1,  -1, 100,  -1,  -1,  -1,-100],\n",
    "                                   [-100, -1, -100,-100,-100,  -1,-100,  -1,-100,-100],\n",
    "                                   [-100, -1,   -1,  -1,  -1,  -1,-100,  -1,  -1,-100],\n",
    "                                   [-100, -1, -100,  -1,-100,  -1,-100,  -1,-100,-100],\n",
    "                                   [-100, -1,   -1,  -1,-100,  -1,  -1,  -1,  -1,-100],\n",
    "                                   [-100,-100,-100,-100,-100,-100,-100,-100,-100,-100]], dtype=torch.int8)\n",
    "\n",
    "stop_matrix = reward_matrix <= -10\n",
    "\"\"\"Bool matrix, True if the cell is a stop cell\n",
    "The barrier cells and the stop cell are stop cells\"\"\"\n",
    "\n",
    "stop_matrix[4, 5] = True        # Set Terminal as stop cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the Q-Matrix\n",
    "\n",
    "4 actions: left, right, up, down.\n",
    "\n",
    "Each action corresponds to a Q-Matrix, and the initial value is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = torch.tensor([[0, -1], [0, 1], [-1, 0], [1, 0]], dtype=torch.int8)\n",
    "n_action = actions.size(0)\n",
    "\n",
    "q_matrix = torch.zeros_like(reward_matrix, dtype=torch.float32).repeat(n_action,1,1)\n",
    "\"\"\"Q Matrix for each action\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning related functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decide the next action\n",
    "\n",
    "Set up the epsilon value, which is used to determine whether to explore or exploit (choose the action with the highest Q-Value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "GREEDY = 0.7\n",
    "\"\"\"The rate of greedy action\"\"\"\n",
    "\n",
    "def choose_action_id(state : torch.Tensor, greedy_rate = GREEDY) -> int:\n",
    "    \"\"\"\n",
    "    Choose an action based on the given state and greedy rate.\n",
    "    \"\"\"\n",
    "    \n",
    "    # ramdom choose action\n",
    "    if random.random() > greedy_rate:\n",
    "        return random.randint(0, n_action - 1)\n",
    "    # greedy choose action\n",
    "    else:\n",
    "        q_values = q_matrix[:, state[0], state[1]] # get q values of all actions\n",
    "        max_value = torch.max(q_values) # get max q value of all actions\n",
    "        max_indices = torch.where(q_values == max_value)[0].tolist() # get indices of max q value\n",
    "        return random.choice(max_indices) # choose one of the max q value\n",
    "    \n",
    "def get_next_state(state : torch.Tensor, action_id : int, actions_ = actions) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Get the next state based on the current state and the action.\n",
    "    \"\"\"\n",
    "    action = actions_[action_id]\n",
    "    next_state = state + action\n",
    "    return next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update Q-Matrix\n",
    "\n",
    "$$\n",
    "Q(s, a) \\leftarrow Q(s, a) + \\alpha \\cdot (R(s, a) + \\gamma \\cdot \\max Q(s', a') - Q(s, a))\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $Q(s, a)$ is the Q-value of action $a$ in state $s$\n",
    "- $\\alpha$ is the learning rate\n",
    "- $R(s, a)$ is the reward of action $a$ in state $s$\n",
    "- $\\gamma$ is the discount factor\n",
    "- $\\max Q(s', a')$ is the maximum Q-value of the next action $a'$ in the next state $s'$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "DISCOUNT = 0.9\n",
    "LR = 0.1\n",
    "\n",
    "def update(state:torch.Tensor, action_id: int, next_state: torch.Tensor, discount = DISCOUNT, lr = LR):\n",
    "    reward = reward_matrix[next_state[0], next_state[1]]\n",
    "    q_old = q_matrix[action_id, state[0], state[1]]\n",
    "    \n",
    "    if stop_matrix[next_state[0], next_state[1]]:\n",
    "        q_new = reward\n",
    "    else:\n",
    "        q_next_max = torch.max(q_matrix[:, next_state[0], next_state[1]])\n",
    "        q_new = reward + discount * q_next_max\n",
    "    \n",
    "    q_matrix[action_id, state[0], state[1]] += lr * (q_new - q_old)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class `Agent`\n",
    "\n",
    "Starting from the initial state, continuously update the Q-Matrix until reaching the target state or exhausting all steps.\n",
    "\n",
    "When hitting a wall, do not update the state. After hitting the wall, try selecting another action from the current state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_STEP = 50\n",
    "\n",
    "def go(start_state: torch.Tensor, valid_mode: bool = False, actions_ = actions):\n",
    "    states = [start_state]\n",
    "    state = states[-1]\n",
    "    while not stop_matrix[state[0], state[1]] and len(states) <= MAX_STEP:\n",
    "        if valid_mode:\n",
    "            action_id = choose_action_id(state, 1)\n",
    "        else:\n",
    "            action_id = choose_action_id(state)\n",
    "        next_state = get_next_state(state, action_id, actions_)\n",
    "        if not valid_mode:\n",
    "            update(state, action_id, next_state)\n",
    "        state = next_state\n",
    "        states.append(state)\n",
    "    return states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define agents\n",
    "\n",
    "Define one or two agents, starting from different initial states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_matrix = torch.zeros_like(q_matrix)\n",
    "\n",
    "state_1 = torch.tensor([8, 1])\n",
    "state_2 = torch.tensor([8, 8]) # test for multiple start points\n",
    "\n",
    "import tqdm\n",
    "for _ in tqdm.trange(500):\n",
    "    go(torch.tensor(state_1))\n",
    "    go(torch.tensor(state_2)) # test for multiple start points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_3 = torch.tensor([8,8])\n",
    "path = go(state_3, True)\n",
    "\n",
    "for step in path:\n",
    "    print(step)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlclass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
