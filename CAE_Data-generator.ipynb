{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from enum import Enum\n",
    "import torch\n",
    "import random\n",
    "import tqdm\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Robot class\n",
    "\n",
    "Given a maze matrix, the robot will start from the start state and go to the target state.\n",
    "\n",
    ".train() method will train the robot.\n",
    ".q_matrix to get the q matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Robot:\n",
    "    \"\"\"\n",
    "    Given a maze matrix, the robot will start from the start state and go to the target state.\n",
    "    \"\"\"\n",
    "    def __init__(self, start_state: tuple[int,int], target_state: tuple[int,int], maze_matrix: torch.Tensor, \n",
    "                 actions: torch.Tensor = torch.tensor([[0, -1], [0, 1], [-1, 0], [1, 0]], dtype=torch.int8),\n",
    "                 reward_matrix: torch.Tensor = None, restrict_matrix: torch.Tensor = None, q_matrix: torch.Tensor = None, \n",
    "                 max_step = 100, greedy_rate = 0.7, discount = 0.9, lr = 0.1, \n",
    "                 wall_reward = -100, target_reward = 100):\n",
    "        \n",
    "        self.start_state = torch.tensor(start_state)\n",
    "        self.target_state = torch.tensor(target_state)\n",
    "        self.maze_matrix = maze_matrix\n",
    "        self.actions = actions\n",
    "        self.max_step = max_step\n",
    "        self.greedy_rate = greedy_rate\n",
    "        self.discount = discount\n",
    "        self.lr = lr\n",
    "        \n",
    "        self.n_action = actions.size(0)\n",
    "        \n",
    "        if reward_matrix is None:\n",
    "            reward_matrix = -torch.ones_like(maze_matrix, dtype=torch.int8)\n",
    "            reward_matrix[maze_matrix] = wall_reward\n",
    "            reward_matrix[target_state[0], target_state[1]] = target_reward\n",
    "        self.reward_matrix = reward_matrix\n",
    "            \n",
    "        if restrict_matrix is None:\n",
    "            restrict_matrix = torch.zeros_like(reward_matrix, dtype=torch.bool)\n",
    "        self.restrict_matrix = restrict_matrix            \n",
    "        \n",
    "        if q_matrix is None:\n",
    "            q_matrix = torch.zeros_like(reward_matrix, dtype=torch.float32).repeat(self.n_action,1,1)\n",
    "        self.q_matrix = q_matrix\n",
    "    \n",
    "    def choose_action_id(self, state: torch.Tensor, greedy_rate: float = None) -> int:\n",
    "        \"\"\"\n",
    "        Choose an action based on the given state and greedy rate.\n",
    "        \"\"\"\n",
    "        \n",
    "        if greedy_rate is None:\n",
    "            greedy_rate = self.greedy_rate\n",
    "            \n",
    "        # ramdom choose action\n",
    "        if random.random() > greedy_rate:\n",
    "            return random.randint(0, self.n_action - 1)\n",
    "        # greedy choose action\n",
    "        else:\n",
    "            q_values = self.q_matrix[:, state[0], state[1]] # get q values of all actions\n",
    "            max_value = torch.max(q_values) # get max q value of all actions\n",
    "            max_indices = torch.where(q_values == max_value)[0].tolist() # get indices of max q value\n",
    "            return random.choice(max_indices) # choose one of the max q value\n",
    "    \n",
    "    def update(self, state:torch.Tensor, action_id: int, next_state: torch.Tensor, discount = None, lr = None):\n",
    "        \"\"\"\n",
    "        Update the q matrix based on the given state, action, next state, discount and learning rate.\n",
    "        \"\"\"\n",
    "        if discount is None:\n",
    "            discount = self.discount\n",
    "        if lr is None:\n",
    "            lr = self.lr\n",
    "        \n",
    "        reward = self.reward_matrix[next_state[0], next_state[1]]\n",
    "        q_old = self.q_matrix[action_id, state[0], state[1]]\n",
    "        \n",
    "        if self.maze_matrix[next_state[0], next_state[1]]:\n",
    "            q_new = reward\n",
    "        else:\n",
    "            q_next_max = torch.max(self.q_matrix[:, next_state[0], next_state[1]])\n",
    "            q_new = reward + discount * q_next_max\n",
    "        \n",
    "        self.q_matrix[action_id, state[0], state[1]] += lr * (q_new - q_old)\n",
    "    \n",
    "    def go(self, start_state: torch.Tensor, valid: bool = False, debug: bool = False) -> list:\n",
    "        \"\"\"\n",
    "        Start from the start state and go to the target state.\n",
    "        \"\"\"\n",
    "        states = [start_state]\n",
    "        state = states[-1]\n",
    "        # while not stop_matrix[state[0], state[1]] and len(states) <= MAX_STEP:\n",
    "        while not torch.allclose(state, self.target_state) and len(states) <= self.max_step:\n",
    "            if valid: # if in valid mode, choose the best action\n",
    "                action_id = self.choose_action_id(state, 1)\n",
    "            else:   # if not in valid mode, choose action based on greedy rate\n",
    "                action_id = self.choose_action_id(state)\n",
    "            # Get next state based on the action\n",
    "            next_state = state + self.actions[action_id]\n",
    "            if not valid: # if not in valid mode, update the q matrix\n",
    "                self.update(state, action_id, next_state)\n",
    "\n",
    "            if not self.maze_matrix[next_state[0], next_state[1]] + self.restrict_matrix[next_state[0], next_state[1]]: # if not hit the wall or restricted\n",
    "                state = next_state\n",
    "            else: # if hit the wall\n",
    "                if debug:\n",
    "                    print(f'{next_state}, hit the wall or restricted')\n",
    "            states.append(next_state) # add the next state to the states list\n",
    "        return states\n",
    "    \n",
    "    def train(self, n_epoch: int = 100, valid: bool = False, debug: bool = False) -> list:\n",
    "        \"\"\"\n",
    "        Train the robot for n_epoch times.\n",
    "        \"\"\"\n",
    "        paths = []\n",
    "        for epoch in tqdm.trange(n_epoch):\n",
    "            path = self.go(self.start_state, valid, debug)\n",
    "            paths.append(path)\n",
    "        return paths\n",
    "\n",
    "    def eval(self, debug: bool = False) -> list:\n",
    "        \"\"\"\n",
    "        Evaluate the robot.\n",
    "        \"\"\"\n",
    "        return self.go(self.start_state, True, debug)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate maze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def randomPrimMaze(width, height) -> torch.Tensor:\n",
    "    \n",
    "    class WALL_DIRECTION(Enum):\n",
    "        WALL_LEFT = 0,\n",
    "        WALL_UP = 1,\n",
    "        WALL_RIGHT = 2,\n",
    "        WALL_DOWN = 3,\n",
    "\n",
    "    class Map():\n",
    "        def __init__(self, width, height):\n",
    "            self.width = width\n",
    "            self.height = height\n",
    "            self.map = torch.ones((height, width), dtype=torch.bool)\n",
    "        \n",
    "        def setEmpty(self, x, y):\n",
    "            self.map[y][x] = False\n",
    "        \n",
    "        def isVisited(self, x, y):\n",
    "            return not self.map[y][x]\n",
    "\n",
    "        def showMap(self):\n",
    "            for row in self.map:\n",
    "                s = ''\n",
    "                for entry in row:\n",
    "                    if not entry:\n",
    "                        s += ' -1\\t'\n",
    "                    elif entry:\n",
    "                        s += ' -100\\t'\n",
    "                    else:\n",
    "                        s += ' X'\n",
    "                print(s)\n",
    "    \n",
    "    # find unvisited adjacent entries of four possible entris\n",
    "    # then add random one of them to checklist and mark it as visited\n",
    "    def checkAdjacentPos(map, x, y, width, height, checklist):\n",
    "        directions = []\n",
    "        if x > 0:\n",
    "            if not map.isVisited(2*(x-1)+1, 2*y+1):\n",
    "                directions.append(WALL_DIRECTION.WALL_LEFT)\n",
    "                    \n",
    "        if y > 0:\n",
    "            if not map.isVisited(2*x+1, 2*(y-1)+1):\n",
    "                directions.append(WALL_DIRECTION.WALL_UP)\n",
    "\n",
    "        if x < width -1:\n",
    "            if not map.isVisited(2*(x+1)+1, 2*y+1):\n",
    "                directions.append(WALL_DIRECTION.WALL_RIGHT)\n",
    "            \n",
    "        if y < height -1:\n",
    "            if not map.isVisited(2*x+1, 2*(y+1)+1):\n",
    "                directions.append(WALL_DIRECTION.WALL_DOWN)\n",
    "            \n",
    "        if len(directions):\n",
    "            direction = random.choice(directions)\n",
    "            #print(\"(%d, %d) => %s\" % (x, y, str(direction)))\n",
    "            if direction == WALL_DIRECTION.WALL_LEFT:\n",
    "                map.setEmpty(2*(x-1)+1, 2*y+1)\n",
    "                map.setEmpty(2*x, 2*y+1)\n",
    "                checklist.append((x-1, y))\n",
    "            elif direction == WALL_DIRECTION.WALL_UP:\n",
    "                map.setEmpty(2*x+1, 2*(y-1)+1)\n",
    "                map.setEmpty(2*x+1, 2*y)\n",
    "                checklist.append((x, y-1))\n",
    "            elif direction == WALL_DIRECTION.WALL_RIGHT:\n",
    "                map.setEmpty(2*(x+1)+1, 2*y+1)\n",
    "                map.setEmpty(2*x+2, 2*y+1)\n",
    "                checklist.append((x+1, y))\n",
    "            elif direction == WALL_DIRECTION.WALL_DOWN:\n",
    "                map.setEmpty(2*x+1, 2*(y+1)+1)\n",
    "                map.setEmpty(2*x+1, 2*y+2 )\n",
    "                checklist.append((x, y+1))\n",
    "            return True\n",
    "        else:\n",
    "            # if not find any unvisited adjacent entry\n",
    "            return False\n",
    "            \n",
    "            \n",
    "    # random prim algorithm\n",
    "    def randomPrim(map, width, height):\n",
    "        startX, startY = (random.randint(0, width-1), random.randint(0, height-1))\n",
    "        print(\"start(%d, %d)\" % (startX, startY))\n",
    "        map.setEmpty(2*startX+1, 2*startY+1)\n",
    "        \n",
    "        checklist = []\n",
    "        checklist.append((startX, startY))\n",
    "        while len(checklist):\n",
    "            # select a random entry from checklist\n",
    "            entry = random.choice(checklist)\t\n",
    "            if not checkAdjacentPos(map, entry[0], entry[1], width, height, checklist):\n",
    "                # the entry has no unvisited adjacent entry, so remove it from checklist\n",
    "                checklist.remove(entry)\n",
    "            \n",
    "    map = Map(width, height)\n",
    "    randomPrim(map, (map.width-1)//2, (map.height-1)//2)\n",
    "    \n",
    "    return map.map\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generae distance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gen_d_matrix(start: tuple[int, int], maze_matrix: torch.Tensor, \n",
    "                 actions: torch.Tensor = torch.tensor([[0, -1], [0, 1], [-1, 0], [1, 0]])) -> torch.Tensor:\n",
    "    start = torch.tensor(start)\n",
    "    \n",
    "    q = deque()\n",
    "    q.append(start)\n",
    "    d_matrix = torch.where(maze_matrix, float('inf'), -1)\n",
    "    d_matrix[start[0],start[1]] = 0\n",
    "    \n",
    "    while q:\n",
    "        state = q.popleft()\n",
    "        for action in actions:\n",
    "            new_state = state + action\n",
    "            if d_matrix[new_state[0],new_state[1]] == -1:\n",
    "                d_matrix[new_state[0],new_state[1]] = d_matrix[state[0],state[1]] + 1\n",
    "                q.append(new_state)\n",
    "    \n",
    "    return d_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get a batch of data\n",
    "\n",
    "### Generate data\n",
    "\n",
    "1. Generate a maze\n",
    "2. Generate a distance matrix (min distance from any start_state)\n",
    "3. For all walkable cells as target_states\n",
    "    - For all start_state, get the q matrices\n",
    "    - For all start_state, get the q matrix_combo (trained by a shared q matrix)\n",
    "\n",
    "Return:\n",
    "- maze_matrix: (height, width)\n",
    "- distance_matrix: (height, width)\n",
    "- q_matrices_dict: {target_states: {start_states: q_matrix}}\n",
    "- q_matrix_combo_dict: {target_states: q_matrix_combo}\n",
    "\n",
    "### Convert data to tensor\n",
    "\n",
    "1. Convert q_matrices_dict to q_matrices_tensor: (B, C, H, W), C = n_action * n_robot\n",
    "2. Convert q_matrix_combo_dict to q_matrix_combo_tensor: (B, C, H, W), C = n_action\n",
    "3. Convert maze_matrix to maze_tensor: (H, W), optional\n",
    "4. Convert distance_matrix to distance_tensor: (H, W), optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gen_data(width: int, height: int, start_states: list[tuple[int,int]]) -> tuple[torch.Tensor, torch.Tensor, dict, dict]:\n",
    "    \"\"\"\n",
    "    Return:\n",
    "    maze_matrix: torch.Tensor, the maze matrix\n",
    "    d_matrix: torch.Tensor, the minimum d matrix among all start states\n",
    "    q_matrices_dict: dict[dict[torch.Tensor]], the q matrix for each target state for each start state\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate maze matrix\n",
    "    maze_matrix = randomPrimMaze(width, height)\n",
    "    \n",
    "    # Generate d matrix for each start state\n",
    "    d_matrices_dict = {}\n",
    "    for start_state in start_states:\n",
    "        d_matrix = gen_d_matrix(start_state, maze_matrix)\n",
    "        d_matrices_dict[start_state] = d_matrix\n",
    "        \n",
    "    # Get min d matrix among all start states\n",
    "    d_matrix = torch.stack(list(d_matrices_dict.values())).min(dim=0).values\n",
    "        \n",
    "    # Generate q matrix for each target state\n",
    "    target_states = torch.nonzero(maze_matrix == False)\n",
    "    target_states = {tuple(target_state.tolist()) for target_state in target_states}\n",
    "    target_states -= set(start_states)\n",
    "    \n",
    "    # print(maze_matrix)\n",
    "    \n",
    "    q_matrices_dict: dict[dict[torch.Tensor]] = {}\n",
    "    q_matrix_combo_dict: dict[torch.Tensor] = {}\n",
    "    \n",
    "    for target_state in target_states:\n",
    "        q_matrices_by_start_dict:dict[torch.Tensor] = {}\n",
    "        \n",
    "        # Generate q matrix for each start state\n",
    "        for start_state in start_states:\n",
    "            # start training\n",
    "            robot = Robot(start_state, target_state, maze_matrix)\n",
    "            robot.train()\n",
    "            \n",
    "            # save trained robot\n",
    "            q_matrices_by_start_dict[start_state] = robot.q_matrix\n",
    "        \n",
    "        # Generate q matrix combo for each target state\n",
    "        q_matrix_combo_dict[target_state] = torch.zeros_like(q_matrices_by_start_dict[start_states[0]])\n",
    "        for start_state in start_states:\n",
    "            # start training\n",
    "            robot = Robot(start_state, target_state, maze_matrix, q_matrix=q_matrices_by_start_dict[start_state])\n",
    "            robot.train()\n",
    "            \n",
    "            # save trained robot\n",
    "            q_matrix_combo_dict[target_state] = robot.q_matrix\n",
    "        \n",
    "        q_matrices_dict[target_state] = q_matrices_by_start_dict\n",
    "        \n",
    "    return maze_matrix, d_matrix, q_matrices_dict, q_matrix_combo_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "def data2tensor(data: tuple[torch.Tensor, torch.Tensor, dict, dict]):\n",
    "    \"\"\"\n",
    "    Convert data to tensor.\n",
    "    1. Convert q_matrices_dict to q_matrices_tensor: (B, C, H, W), C = n_action * n_robot\n",
    "    2. Convert q_matrix_combo_dict to q_matrix_combo_tensor: (B, C, H, W), C = n_action\n",
    "    3. Convert maze_matrix to maze_tensor: (H, W), optional\n",
    "    4. Convert distance_matrix to distance_tensor: (H, W), optional\n",
    "    \"\"\"\n",
    "    maze_matrix, d_matrix, q_matrices_dict, q_matrix_combo_dict = data\n",
    "    maze_tensor = maze_matrix.to(device)\n",
    "    d_tensor = d_matrix.to(device)\n",
    "    \n",
    "    q_matrices_tensor = torch.stack([torch.cat(list(q_matrices_dict[target_state].values())) for target_state in q_matrices_dict]).to(device)\n",
    "    \n",
    "    q_matrix_combo_tensor = torch.stack(list(q_matrix_combo_dict.values())).to(device)\n",
    "    \n",
    "    target_states = torch.tensor(list(q_matrices_dict.keys()), dtype=torch.int8).to(device)\n",
    "    \n",
    "    start_states = torch.tensor(list(list(q_matrices_dict.values())[0].keys()), dtype=torch.int8).to(device)\n",
    "    \n",
    "    return maze_tensor, d_tensor, q_matrices_tensor, q_matrix_combo_tensor, target_states, start_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = []\n",
    "n_batch = 100\n",
    "for _ in range(n_batch):\n",
    "    batch = data2tensor(gen_data(9,9,[(7,1),(7,7)]))\n",
    "    batches.append(batch)\n",
    "\n",
    "torch.save(batches, 'batches.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlclass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
