{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the sample maze\n",
    "\n",
    "Set up a maze in size of 10 × 10, and represent the maze in reward matrix. The reward matrix is a 10 × 10 matrix, and the reward of each grid is as follows:\n",
    "\n",
    "- The reward of the grids in the border is -100.\n",
    "- The reward of the grids in the middle is -1.\n",
    "- The reward of the destination is 100.\n",
    "\n",
    "Set up a stop matrix, which is a 10 × 10 bool matrix. The stop matrix is used to determine whether the agent can stop at the grid. \n",
    "\n",
    "The end point is (4,5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_matrix = torch.tensor(data=[[-100,-100,-100,-100,-100,-100,-100,-100,-100,-100],\n",
    "                                   [-100, -1, -100,  -1,-100,  -1,-100,  -1,-100,-100],\n",
    "                                   [-100, -1,   -1,  -1,  -1,  -1,-100,  -1,  -1,-100],\n",
    "                                   [-100, -1, -100,  -1,-100,  -1,-100,  -1,-100,-100],\n",
    "                                   [-100, -1,   -1,  -1,  -1, 100,  -1,  -1,  -1,-100],\n",
    "                                   [-100, -1, -100,-100,-100,  -1,-100,  -1,-100,-100],\n",
    "                                   [-100, -1,   -1,  -1,  -1,  -1,-100,  -1,  -1,-100],\n",
    "                                   [-100, -1, -100,  -1,-100,  -1,-100,  -1,-100,-100],\n",
    "                                   [-100, -1,   -1,  -1,-100,  -1,  -1,  -1,  -1,-100],\n",
    "                                   [-100,-100,-100,-100,-100,-100,-100,-100,-100,-100]], dtype=torch.int8)\n",
    "\n",
    "stop_matrix = reward_matrix <= -100\n",
    "\"\"\"Bool matrix, True if the cell is a stop cell\n",
    "The barrier cells and the stop cell are stop cells\"\"\"\n",
    "\n",
    "target_index = torch.tensor([4,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the Q-Matrix\n",
    "\n",
    "4 actions: left, right, up, down.\n",
    "\n",
    "Each action corresponds to a Q-Matrix, and the initial value is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = torch.tensor([[0, -1], [0, 1], [-1, 0], [1, 0]], dtype=torch.int8)\n",
    "n_action = actions.size(0)\n",
    "\n",
    "q_matrix = torch.zeros_like(reward_matrix, dtype=torch.float32).repeat(n_action,1,1)\n",
    "\"\"\"Q Matrix for each action\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning related functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decide the next action\n",
    "\n",
    "Set up the epsilon value, which is used to determine whether to explore or exploit (choose the action with the highest Q-Value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "GREEDY = 0.7\n",
    "\"\"\"The rate of greedy action\"\"\"\n",
    "\n",
    "def choose_action_id(state : torch.Tensor, greedy_rate = GREEDY) -> int:\n",
    "    \"\"\"\n",
    "    Choose an action based on the given state and greedy rate.\n",
    "    \"\"\"\n",
    "    \n",
    "    # ramdom choose action\n",
    "    if random.random() > greedy_rate:\n",
    "        return random.randint(0, n_action - 1)\n",
    "    # greedy choose action\n",
    "    else:\n",
    "        q_values = q_matrix[:, state[0], state[1]] # get q values of all actions\n",
    "        max_value = torch.max(q_values) # get max q value of all actions\n",
    "        max_indices = torch.where(q_values == max_value)[0].tolist() # get indices of max q value\n",
    "        return random.choice(max_indices) # choose one of the max q value\n",
    "    \n",
    "def get_next_state(state : torch.Tensor, action_id : int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Get the next state based on the current state and the action.\n",
    "    \"\"\"\n",
    "    action = actions[action_id]\n",
    "    next_state = state + action\n",
    "    return next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update Q-Matrix\n",
    "\n",
    "$$\n",
    "Q(s, a) \\leftarrow Q(s, a) + \\alpha \\cdot (R(s, a) + \\gamma \\cdot \\max Q(s', a') - Q(s, a))\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $Q(s, a)$ is the Q-value of action $a$ in state $s$\n",
    "- $\\alpha$ is the learning rate\n",
    "- $R(s, a)$ is the reward of action $a$ in state $s$\n",
    "- $\\gamma$ is the discount factor\n",
    "- $\\max Q(s', a')$ is the maximum Q-value of the next action $a'$ in the next state $s'$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DISCOUNT = 0.9\n",
    "LR = 0.1\n",
    "\n",
    "def update(state:torch.Tensor, action_id: int, next_state: torch.Tensor, discount = DISCOUNT, lr = LR):\n",
    "    reward = reward_matrix[next_state[0], next_state[1]]\n",
    "    q_old = q_matrix[action_id, state[0], state[1]]\n",
    "    \n",
    "    if stop_matrix[next_state[0], next_state[1]]:\n",
    "        q_new = reward\n",
    "    else:\n",
    "        q_next_max = torch.max(q_matrix[:, next_state[0], next_state[1]])\n",
    "        q_new = reward + discount * q_next_max\n",
    "    \n",
    "    q_matrix[action_id, state[0], state[1]] += lr * (q_new - q_old)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class `Agent`\n",
    "\n",
    "Starting from the initial state, continuously update the Q-Matrix until reaching the target state or exhausting all steps.\n",
    "\n",
    "When hitting a wall, do not update the state. After hitting the wall, try selecting another action from the current state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_STEP = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, start_state: torch.Tensor, restriction_matrix: torch.Tensor, max_step = MAX_STEP):\n",
    "        self.state = start_state\n",
    "        self.restriction_matrix = restriction_matrix\n",
    "        self.max_step = max_step\n",
    "        self.states = [start_state]\n",
    "        \n",
    "    def go(self, start_state: torch.Tensor, valid: bool = False, debug: bool = False) -> list:\n",
    "        \"\"\"\n",
    "        Start from the start state and go to the target state.\n",
    "        \"\"\"\n",
    "        states = [start_state]\n",
    "        state = states[-1]\n",
    "        # while not stop_matrix[state[0], state[1]] and len(states) <= MAX_STEP:\n",
    "        while not torch.allclose(state, target_index) and len(states) <= MAX_STEP:\n",
    "            if valid: # if in valid mode, choose the best action\n",
    "                action_id = choose_action_id(state, 1)\n",
    "            else:   # if not in valid mode, choose action based on greedy rate\n",
    "                action_id = choose_action_id(state)\n",
    "            # Get next state based on the action\n",
    "            next_state = get_next_state(state, action_id)\n",
    "            if not valid: # if not in valid mode, update the q matrix\n",
    "                update(state, action_id, next_state)\n",
    "\n",
    "            if not stop_matrix[next_state[0], next_state[1]] + self.restriction_matrix[next_state[0], next_state[1]]: # if not hit the wall or restricted\n",
    "                state = next_state\n",
    "            else: # if hit the wall\n",
    "                if debug:\n",
    "                    print(f'{next_state}, hit the wall or restricted')\n",
    "            states.append(next_state) # add the next state to the states list\n",
    "        return states\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define agents\n",
    "\n",
    "Define 4 agents, and restrict them in areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_matrix = torch.zeros_like(q_matrix)\n",
    "\n",
    "restriction_matrix_1 = torch.zeros_like(reward_matrix, dtype=torch.bool)\n",
    "restriction_matrix_1[6, :] = 1; restriction_matrix_1[:,6] = 1\n",
    "\n",
    "restriction_matrix_2 = torch.zeros_like(reward_matrix, dtype=torch.bool)\n",
    "restriction_matrix_2[6, :] = 1; restriction_matrix_2[:,3] = 1\n",
    "\n",
    "restriction_matrix_3 = torch.zeros_like(reward_matrix, dtype=torch.bool)\n",
    "restriction_matrix_3[3, :] = 1; restriction_matrix_3[:,6] = 1\n",
    "\n",
    "restriction_matrix_4 = torch.zeros_like(reward_matrix, dtype=torch.bool)\n",
    "restriction_matrix_4[3, :] = 1; restriction_matrix_4[:,3] = 1\n",
    "\n",
    "\n",
    "state_1 = torch.tensor([1, 1]) # start state 1\n",
    "state_2 = torch.tensor([2, 8]) # start state 2\n",
    "state_3 = torch.tensor([8, 1]) # start state 3\n",
    "state_4 = torch.tensor([8, 8]) # start state 4\n",
    "\n",
    "r1 = Agent(state_1, restriction_matrix_1)\n",
    "r2 = Agent(state_2, restriction_matrix_2)\n",
    "r3 = Agent(state_3, restriction_matrix_3)\n",
    "r4 = Agent(state_4, restriction_matrix_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "for _ in tqdm.trange(100):\n",
    "    r1.go(state_1)\n",
    "    r2.go(state_2)\n",
    "    r3.go(state_3)\n",
    "    r4.go(state_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test one agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restriction_matrix_test = torch.zeros_like(reward_matrix, dtype=torch.bool) # disable all restrictions\n",
    "state_test = torch.tensor([6, 8]) # start state\n",
    "\n",
    "r_test = Agent(state_test, restriction_matrix_test) # create an agent for testing\n",
    "path = r_test.go(state_test, True) # get the path from the start state to the target state\n",
    "\n",
    "last_pos = path[0]\n",
    "for step in path:\n",
    "    if not stop_matrix[step[0], step[1]]:\n",
    "        last_pos = step\n",
    "        print(step)\n",
    "    else:\n",
    "        print(f'{step}, hit the wall this time, back to pos: {last_pos}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test crossing areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_path_staging(start_state : torch.Tensor, target_state: torch.Tensor) -> list:\n",
    "    restriction_matrix = torch.zeros_like(reward_matrix, dtype=torch.bool)\n",
    "    r_start = Agent(start_state, restriction_matrix)\n",
    "    path_from_start = r_start.go(start_state, True)\n",
    "    \n",
    "    r_target = Agent(target_state, restriction_matrix)\n",
    "    path_from_target = r_target.go(target_state, True)\n",
    "    \n",
    "    return path_from_start[:-1] + list(reversed(path_from_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_path_staging(torch.tensor([6,8]), torch.tensor([4,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_path_staging(torch.tensor([3,1]), torch.tensor([2,8]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlclass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
